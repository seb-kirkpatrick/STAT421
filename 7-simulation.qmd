---
title: "Simulation"
author: "Gregory J. Matthews"
format: 
  revealjs:
    chalkboard: true
    slide-number: c/t
    code-line-numbers: false
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
editor: visual
execute: 
  echo: true
---

## I need to show you this
```{r}
#| eval: false
library(tidyverse)
#devtools::install_github("brooke-watson/BRRR")
f <- function(sound, sleep = 0.75){
Sys.sleep(sleep)
BRRR::skrrrahh(sound)
}

for (i in 1:5 ){
f(i)
}
```

## Why simulation?
 - Weâ€™re in the 21st century!
 - Simulations can often be easier than hand calculations 
 - R provides unique access to great (statistical) simulation tools (compared to other languages)
 
![](old-dogs.png) 

## 
Basic structure of stochastic simulations

 - Identify a random variable of interest $X$ and write a program
to simulate it.
 - Generate an iid sample $X_1 \cdots X_n$ with the same distribution
as $X$.
 - Estimate $E[X]$ (using $\bar{X}$) and assess the accuracy of the
estimate (using a confidence interval).

## Small Example Simulation Study
Mosteller, #25: If a chord is selected at random on a fixed circle, what is the probability that the its length exceeds the radius of a circle?  
```{r}
chordlength <- function(){
theta <- runif(2, 0,2*pi)
r <- 1
x <- r*cos(theta)
y <- r*sin(theta)
return(sqrt((x[2]-x[1])^2 + (y[2]-y[1])^2))
}
chordlength()

mosteller <- replicate(100000,chordlength())
mean(mosteller > 1) 
```


## Model building
 - That first step is an example of model building
 - Random variables are the building blocks of these models
 - R provides us with some random variables, but let's discuss
how to create our own
 - All random variables can be generated by manipulating
Uniform(0,1) random variables, so we start there.

## Generating Random Numbers
 - You can't generate truly random numbers on a computer.
 - So we generate pseudo-random numbers
 - These appear random, but are actually deterministic.
 - Because they are deterministic, experiments can be repeated
exactly

## Mersenne-Twister
 - R uses a pseudo-random number generator called the
**Mersenne Twister**
 - Given some initial number $X_0 \in 0, 1, \cdots, m-1$ and two big
numbers $A$ and $B$:

$$
X_{n+1} = (AX_n + B)\mod{m}
$$

## Mersenne Twister
```{r}
#Check the method that R is using for random number generation.  
RNGkind()
```

## Mersenne-Twister
 - We can take this sequence and get $U_n \in [0, 1)$ by taking $\frac{X_n}{m}$.
 - If $m$, $A$, and $B$ are well chosen then the sequence $U_0,U_1, \cdots, U_n$
is almost impossible to distinguish from an iid sequence of
U(0, 1) random variables.
  - Good choices for m, A, B:
    - m = 232
    - A = 1664525
    - B = 1013904223
    
## Mersenne-Twister: A note
 - RANDU was used in IBM computers in the 1970's
 - Bad choices for m, A, B:
    - m = 231
    - A = 65539
    - B = 0
    
## Mersenne-Twister: Let's do it
Let's construct a random number generator with $m$ = 10,
$A$ = 103, and $B$ = 17 with $X_0$ = 2.

## Mersenne-Twister: Let's do it
Write code here.  
Plot a histogram, time series plot, and scatter plot.  

## 
 - $X_0$ is called the seed. If we know this and $A$, $B$, and $m$ we can
reproduce the whole sequence of random numbers.
- In R you can generate n random uniform variable with
runif(n)
 - If you don't set a seed, R initializes the pseudo-random
number generator with the system clock.

##
```{r}
set.seed(42)
runif(2)

RNG.state <- .Random.seed
runif(2)

set.seed(42)
runif(4)

.Random.seed <- RNG.state
runif(2)
```

## 
 - `.Random.seed` is an integer vector, containing the random number generator (RNG) state for random number generation in R. It can be saved and restored, but should not be altered by the user.
 - `RNGkind` is a more friendly interface to query or set the kind of RNG in use.
 - `set.seed` is the recommended way to specify seeds.

## Simulating discrete random variables
 - Once we are able to sample from a uniform random variable
we can sample from any random variable that we want.
 - How can we do this?
 - Let's take a look at a pmf for a binomial random variable.

##
```{r}
#| echo: false
library(tidyverse)
plot(stepfun(0:3,c(0,pbinom(0:3,3,0.6))))
```

##
```{r}
#| echo: false
plot(stepfun(0:3,c(0,pbinom(0:3,3,0.6))))
points(c(-2,2),c(0.5,0.5),type = "l", col = "red")
```

##
```{r}
#| echo: false
plot(stepfun(0:3,c(0,pbinom(0:3,3,0.6))))
points(c(-2,2),c(0.5,0.5),type = "l", col = "red")
points(c(2,2),c(0.5,0),type = "l", col = "red")
points(c(2),c(0), col = "red",pch = 16)
```

##
 - This is the inverse cdf!
 - We have a function for that in R.  
 - For binomial is qbinom.  
```{r}
set.seed(1234)
u <- runif(10)
bin <- qbinom(u, 3, 0.6)
bin
```

##
```{r}
set.seed(1234)
u <- runif(1000)
bin <- qbinom(u, 3, 0.6)
table(bin)/1000
#True probabilies 
dbinom(0:3,3,0.6)
```

## Inversion Method
::: {.smaller}
Let $U \sim U(0,1)$ and we want to simulated from a r.v. $X$ with cdf $F_{X}(x)$.  Define $Y = F_{X}^{-1}(U)$, then
$$
F_{Y}(y) = P(Y \le y) = P(F_{X}^{-1}(U) \le y) 
$$

$$
= P(U \le F_{X}(y)) = F_X(y).  
$$
That is $Y$ has the same distribution as $X$. So if we can simulate from a $U(0,1)$ we can simulate from any continuous rv $X$ for which we know $F^{-1}_X$.  This is called the inversion method.  
Note: For any continuous rv $F_X(X) \sim U(0,1)$
:::

## Exponential Distribution Example
Let $X \sim Exp(\theta)$. 

$$
f_X(x) = \frac{1}{\theta}e^{\frac{-x}{\theta}}
$$

$$
F_X(x) = 1-e^{\frac{-x}{\theta}} I(x \ge 0)
$$

Set
$$
y = 1 - e^{\frac{-x}{\theta}}
$$
$$
x = -\theta log(1-y) = F_{X}^{-1}(y)
$$
## 
```{r}
u <- runif(5)
Finv <- function(y,theta=1){-theta*log(1-y)}
Finv(u)
set.seed(1234)
u <- runif(1000)
Finv <- function(u,theta=1){-theta*log(1-u)}
#Could also use Finv <- function(u,theta=1){-theta*log(u)}.  Why?
y <- Finv(u)
```

##
```{r}
#| echo: false
library(ggplot2)
dat <- data.frame(y)
ggplot(aes(x = y), data = dat) + geom_histogram() + theme_bw()
```

##
```{r}
#really close!
truth <- data.frame(x = seq(0,7,0.001),y = pexp(seq(0,7,0.001)))
ggplot(aes(x = y), data = dat) + stat_ecdf() + theme_bw() + geom_path(aes(x=x,y=y), data = truth, color = "red") 
#Base R
#plot(ecdf(y))
#points(seq(0,7,0.001),pexp(seq(0,7,0.001)),type="l",col="red")
```

## Normal Example
```{r}
set.seed(1234)
u <- runif(1000)
x <- qnorm(u)
```

## 
```{r}
#| echo: false
qqnorm(x)
```

## What if we don't have qnorm function?
```{r}
#pdf
pdfnorm <- function(x, mu = 0, sigma = 1){
  out <- 1/(sigma*sqrt(2*pi)) * exp(-1/(2*sigma^2)*(x - mu)^2)  
  return(out)
}
#cdf
cdfnorm <- function(x, mu = 0, sigma = 1){
  sum(pdfnorm(seq(-10*sigma,x,0.00001))*0.00001)
}
```

## What if we don't have qnorm function?
```{r}
#inverse cdf
icdfnorm <- function(p, mu = 0, sigma = 1){
  #Objective function

  f <- function(x,p){
  abs(cdfnorm(x) - p)
  }

  optimize(f, lower = -10, upper = 10, p=p)$minimum
    }

icdfnorm <- Vectorize(icdfnorm)
u <- runif(10)
icdfnorm(u)
```

## Rejection method for continuosu RV

```{r}
RNGkind()
```
 - Rejection method for contionuous rv
 - Let's do triangle distribution example.
 
 
## Triangle Distribution
```{r}
#this function is the pdf of a triangle distribution
triangle <- function(x){
  if (x < 1 & x > 0){
    return(x)
  }else if(x >= 1 & x < 2){
    return(2-x)
  } else {
      return(0)
    }
}

triangle <- Vectorize(triangle)
```



##
```{r}
#| echo: false
set.seed(1234)
x <- runif(1000,0,2)
y <- runif(1000,0,2)
col <- rep("black",1000)
col[y < triangle(x)] <- "red"
plot(x,y,ylim=c(0,2),xlim=c(0,2), pch = 16, cex = 0.5)
points(seq(0,2,0.001),triangle(seq(0,2,0.001)),type= "l",col = "red", lwd = 4)
```

##
```{r}
#| echo: false
set.seed(1234)
x <- runif(1000,0,2)
y <- runif(1000,0,2)
col <- rep("black",1000)
col[y < triangle(x)] <- "red"
plot(x,y,ylim=c(0,2),xlim=c(0,2), col = col, pch = 16, cex = 0.5)
points(seq(0,2,0.001),triangle(seq(0,2,0.001)),type= "l",col = "red", lwd = 4)
```
##
```{r}
#randomly sample from a triangle distribution
samp <- function(n,k=2){
#set.seed(4321)
  randsamp <- c()
  while (length(randsamp) < n){
x <- runif(1,0,2)
y <- runif(1,0,k)
  if (y <= triangle(x)){
   randsamp <- c(randsamp,x)
  }
}

return(randsamp)
}
```

## 
```{r}
set.seed(1234)
data.frame(x = samp(10000,k=2)) %>% 
  ggplot(aes(x = x)) + 
  geom_histogram() + 
  theme_bw()
```


## Rejection method in general
 - Notice that in our example though the support of the r.v. was finite.  
 - What can we do if we have a r.v. with support on say the real numbers?  
 - We can't use a uniform envelope because it would have infinite area.  
 - Instead we need a shape with finite area
 
## Rejection method in general
- Consider a pdf $h$ for a r.v. $X$ and let $Y\sim U(0, kh(X))$.  This means that the upper bound of $Y$ depends on $X$.  
- Suppose we have a target pdf we want to sample from named $f_{X}$
.  
- If we choose $k \ge k^{\star} = sup_x \frac{f_X(x)}{h(x)}$ then $kh(x)$ forms and enevlope for $f_X(x)$.   
- So we accept candidate points below $f_X(x)$ and reject everything else. 

## Steps in general
To simulate from the density $f_X$, we assume that we have envelope density $h$ from which to simulate, and that we have some $k$ < $\infty$ such that $sup_x \frac{f_X(x)}{h(x)} \le k$

 1. Simulated $X$ from $h$.
 2. Generate $Y\sim U(0,kh(X))$
 3. If $Y \le f_X(x)$ then return $X$, otherwise go back to step 1.  

## A note on efficiency
- Efficiency is measured here by the expected number of candidate draws. 
- The area under $kh$ is $k$ and the area under $f_X$ is 1.  So the probability of accepting a candidate is $\frac{1}{k}$
- The number of times $N$ we have to generate a candidate point has distribution 1 + geom($\frac{1}{k}$).  
- This has expected value $k$, thuse the closer $h$ is to $f_X$, the smaller $k$ needs to be and the more efficient the algorithm is.  