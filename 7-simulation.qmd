---
title: "Simulation"
author: "Gregory J. Matthews"
format: 
  revealjs:
    chalkboard: true
    slide-number: c/t
    code-line-numbers: false
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
editor: visual
execute: 
  echo: true
---

## I need to show you this

```{r}
#| eval: false
library(tidyverse)
#devtools::install_github("brooke-watson/BRRR")
f <- function(sound, sleep = 0.75){
Sys.sleep(sleep)
BRRR::skrrrahh(sound)
}

for (i in 1:5 ){
f(i)
}
```

## Why simulation?

-   Weâ€™re in the 21st century!
-   Simulations can often be easier than hand calculations
-   R provides unique access to great (statistical) simulation tools (compared to other languages)

![](old-dogs.png)

## 

Basic structure of stochastic simulations

-   Identify a random variable of interest $X$ and write a program to simulate it.
-   Generate an iid sample $X_1 \cdots X_n$ with the same distribution as $X$.
-   Estimate $E[X]$ (using $\bar{X}$) and assess the accuracy of the estimate (using a confidence interval).

## Small Example Simulation Study

Mosteller, #25: If a chord is selected at random on a fixed circle, what is the probability that the its length exceeds the radius of a circle?

```{r}
chordlength <- function(){
theta <- runif(2, 0,2*pi)
r <- 1
x <- r*cos(theta)
y <- r*sin(theta)
return(sqrt((x[2]-x[1])^2 + (y[2]-y[1])^2))
}
chordlength()

mosteller <- replicate(100000,chordlength())
mean(mosteller > 1) 
```

## Model building

-   That first step is an example of model building
-   Random variables are the building blocks of these models
-   R provides us with some random variables, but let's discuss how to create our own
-   All random variables can be generated by manipulating Uniform(0,1) random variables, so we start there.

## Generating Random Numbers

-   You can't generate truly random numbers on a computer.
-   So we generate pseudo-random numbers
-   These appear random, but are actually deterministic.
-   Because they are deterministic, experiments can be repeated exactly

## Mersenne-Twister

-   R uses a pseudo-random number generator called the **Mersenne Twister**
-   Given some initial number $X_0 \in 0, 1, \cdots, m-1$ and two big numbers $A$ and $B$:

$$
X_{n+1} = (AX_n + B)\mod{m}
$$

## Mersenne Twister

```{r}
#Check the method that R is using for random number generation.  
RNGkind()
```

## Mersenne-Twister

-   We can take this sequence and get $U_n \in [0, 1)$ by taking $\frac{X_n}{m}$.
-   If $m$, $A$, and $B$ are well chosen then the sequence $U_0,U_1, \cdots, U_n$ is almost impossible to distinguish from an iid sequence of U(0, 1) random variables.
-   Good choices for m, A, B:
    -   m = 232
    -   A = 1664525
    -   B = 1013904223

## Mersenne-Twister: A note

-   RANDU was used in IBM computers in the 1970's
-   Bad choices for m, A, B:
    -   m = 231
    -   A = 65539
    -   B = 0

## Mersenne-Twister: Let's do it

Let's construct a random number generator with $m$ = 10, $A$ = 103, and $B$ = 17 with $X_0$ = 2.

## Mersenne-Twister: Let's do it

Write code here.\
Plot a histogram, time series plot, and scatter plot.

```{r}
nsim <- 100000
x <- rep(NA, nsim)
x[1] <- 2
A <- 172541
B <- 32657483
m <- 174

for (i in 2:nsim){
x[i] <- (A*x[i-1] + B) %% m
}

hist(x/(m-1))


x <- runif(1000)
acf(x)


```

## 

-   $X_0$ is called the seed. If we know this and $A$, $B$, and $m$ we can reproduce the whole sequence of random numbers.
-   In R you can generate n random uniform variable with runif(n)
-   If you don't set a seed, R initializes the pseudo-random number generator with the system clock.

## 

```{r}
set.seed(42)
runif(2)

RNG.state <- .Random.seed
runif(2)

set.seed(42)
runif(4)

.Random.seed <- RNG.state
runif(2)
```

## 

-   `.Random.seed` is an integer vector, containing the random number generator (RNG) state for random number generation in R. It can be saved and restored, but should not be altered by the user.
-   `RNGkind` is a more friendly interface to query or set the kind of RNG in use.
-   `set.seed` is the recommended way to specify seeds.

## Simulating discrete random variables

-   Once we are able to sample from a uniform random variable we can sample from any random variable that we want.
-   How can we do this?
-   Let's take a look at a cdf for a binomial random variable.

## 

```{r}
#| echo: false
library(tidyverse)
plot(stepfun(0:3,c(0,pbinom(0:3,3,0.6))))
```

## 

```{r}
#| echo: false
plot(stepfun(0:3,c(0,pbinom(0:3,3,0.6))))
points(c(-2,2),c(0.5,0.5),type = "l", col = "red")
```

## 

```{r}
#| echo: false
plot(stepfun(0:3,c(0,pbinom(0:3,3,0.6))))
points(c(-2,2),c(0.5,0.5),type = "l", col = "red")
points(c(2,2),c(0.5,0),type = "l", col = "red")
points(c(2),c(0), col = "red",pch = 16)
```

## 

-   This is the inverse cdf!
-   We have a function for that in R.\
-   For binomial is qbinom.\

```{r}
set.seed(1234)
u <- runif(10)
bin <- qbinom(u, 3, 0.6)
bin
```

## 

```{r}
set.seed(1234)
u <- runif(1000)
bin <- qbinom(u, 3, 0.6)
table(bin)/1000
#True probabilies 
dbinom(0:3,3,0.6)
```

## Inversion Method

::: smaller
Let $U \sim U(0,1)$ and we want to simulated from a r.v. $X$ with cdf $F_{X}(x)$. Define $Y = F_{X}^{-1}(U)$, then $$
F_{Y}(y) = P(Y \le y) = P(F_{X}^{-1}(U) \le y) 
$$

$$
= P(U \le F_{X}(y)) = F_X(y).  
$$ That is $Y$ has the same distribution as $X$. So if we can simulate from a $U(0,1)$ we can simulate from any continuous rv $X$ for which we know $F^{-1}_X$. This is called the inversion method.\
Note: For any continuous rv $F_X(X) \sim U(0,1)$
:::

## Exponential Distribution Example

Let $X \sim Exp(\theta)$.

$$
f_X(x) = \frac{1}{\theta}e^{\frac{-x}{\theta}}
$$

$$
F_X(x) = 1-e^{\frac{-x}{\theta}} I(x \ge 0)
$$

Set $$
y = 1 - e^{\frac{-x}{\theta}}
$$ $$
x = -\theta log(1-y) = F_{X}^{-1}(y)
$$

## 

```{r}
u <- runif(5)
Finv <- function(y,theta=1){-theta*log(1-y)}
Finv(u)
set.seed(1234)
u <- runif(1000)
Finv <- function(u,theta=1){-theta*log(1-u)}
#Could also use Finv <- function(u,theta=1){-theta*log(u)}.  Why?
y <- Finv(u)
```

## 

```{r}
#| echo: false
library(ggplot2)
dat <- data.frame(y)
ggplot(aes(x = y), data = dat) + geom_histogram() + theme_bw()
```

## 

```{r}
#really close!
truth <- data.frame(x = seq(0,7,0.001),y = pexp(seq(0,7,0.001)))
ggplot(aes(x = y), data = dat) + stat_ecdf() + theme_bw() + geom_path(aes(x=x,y=y), data = truth, color = "red") 
#Base R
#plot(ecdf(y))
#points(seq(0,7,0.001),pexp(seq(0,7,0.001)),type="l",col="red")
```

## Normal Example

```{r}
set.seed(1234)
u <- runif(1000)
x <- qnorm(u)
```

## 

```{r}
#| echo: false
qqnorm(x)
```

## What if we don't have qnorm function?

```{r}
#pdf
pdfnorm <- function(x, mu = 0, sigma = 1){
  out <- 1/(sigma*sqrt(2*pi)) * exp(-1/(2*sigma^2)*(x - mu)^2)  
  return(out)
}
#cdf
cdfnorm <- function(x, mu = 0, sigma = 1){
  sum(pdfnorm(seq(-10*sigma,x,0.00001))*0.00001)
}
```

## What if we don't have qnorm function?

```{r}
#inverse cdf
icdfnorm <- function(p, mu = 0, sigma = 1){
  #Objective function

  f <- function(x,p){
  abs(cdfnorm(x) - p)
  }

  optimize(f, lower = -10, upper = 10, p=p)$minimum
    }

icdfnorm <- Vectorize(icdfnorm)
u <- runif(10)
icdfnorm(u)
```

## Rejection method for continuosu RV

```{r}
RNGkind()
```

-   Rejection method for contionuous rv
-   Let's do triangle distribution example.

## Triangle Distribution

```{r}
#this function is the pdf of a triangle distribution
triangle <- function(x){
  if (x < 1 & x > 0){
    return(x)
  }else if(x >= 1 & x < 2){
    return(2-x)
  } else {
      return(0)
    }
}

triangle <- Vectorize(triangle)
```

## 

```{r}
#| echo: false
set.seed(1234)
x <- runif(1000,0,2)
y <- runif(1000,0,2)
col <- rep("black",1000)
col[y < triangle(x)] <- "red"
plot(x,y,ylim=c(0,2),xlim=c(0,2), pch = 16, cex = 0.5)
points(seq(0,2,0.001),triangle(seq(0,2,0.001)),type= "l",col = "red", lwd = 4)
```

## 

```{r}
#| echo: false
set.seed(1234)
x <- runif(1000,0,2)
y <- runif(1000,0,2)
col <- rep("black",1000)
col[y < triangle(x)] <- "red"
plot(x,y,ylim=c(0,2),xlim=c(0,2), col = col, pch = 16, cex = 0.5)
points(seq(0,2,0.001),triangle(seq(0,2,0.001)),type= "l",col = "red", lwd = 4)
```

## 

```{r}
#randomly sample from a triangle distribution
samp <- function(n,k=2){
#set.seed(4321)
  randsamp <- c()
  while (length(randsamp) < n){
x <- runif(1,0,2)
y <- runif(1,0,k)
  if (y <= triangle(x)){
   randsamp <- c(randsamp,x)
  }
}

return(randsamp)
}
```

## 

```{r}
set.seed(1234)
data.frame(x = samp(10000,k=2)) %>% 
  ggplot(aes(x = x)) + 
  geom_histogram() + 
  theme_bw()
```

## Rejection method in general

-   Notice that in our example though the support of the r.v. was finite.\
-   What can we do if we have a r.v. with support on say the real numbers?\
-   We can't use a uniform envelope because it would have infinite area.\
-   Instead we need a shape with finite area

## Rejection method in general

-   Consider a pdf $h$ for a r.v. $X$ and let $Y\sim U(0, kh(X))$. This means that the upper bound of $Y$ depends on $X$.\
-   Suppose we have a target pdf we want to sample from named $f_{X}$ .\
-   If we choose $k \ge k^{\star} = sup_x \frac{f_X(x)}{h(x)}$ then $kh(x)$ forms and enevlope for $f_X(x)$.\
-   So we accept candidate points below $f_X(x)$ and reject everything else.

## Steps in general

To simulate from the density $f_X$, we assume that we have envelope density $h$ from which to simulate, and that we have some $k$ \< $\infty$ such that $sup_x \frac{f_X(x)}{h(x)} \le k$

1.  Simulated $X$ from $h$.
2.  Generate $Y\sim U(0,kh(X))$
3.  If $Y \le f_X(x)$ then return $X$, otherwise go back to step 1.

## A note on efficiency

-   Efficiency is measured here by the expected number of candidate draws.
-   The area under $kh$ is $k$ and the area under $f_X$ is 1. So the probability of accepting a candidate is $\frac{1}{k}$
-   The number of times $N$ we have to generate a candidate point has distribution 1 + geom($\frac{1}{k}$).\
-   This has expected value $k$, thuse the closer $h$ is to $f_X$, the smaller $k$ needs to be and the more efficient the algorithm is.

## Gamma

-   Let's say we want to simulate from a Gamma($\alpha$, $\beta$).\
-   $f(x) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}$ for $x > 0$.
-   There is no closed form expression for the CDF of a gamma distribution.

## Gamma

\item

We will use an exponential envelope. So $h(x) = \lambda e^{-\lambda x}$ for $x>0$.\

\item

$h$ is easy to sample from using the inversion method with $\frac{-log(U)}{\lambda}$ where $U$ is Uniform(0,1)

\item

Now we need to find $k^{\star}$

## Gamma

$$
k^{\star} = sup_{x>0} \frac{f(x)}{h(x)} = sup_{x>0} \frac{\beta^\alpha x^{\alpha-1}e^{(\lambda-\beta)x}}{\lambda \Gamma(\alpha)}
$$ If $\alpha<1$ or $\beta < \lambda$, $k^{\star}$ will be infinite. \\ $$
x = \frac{\alpha-1}{\beta-\lambda}
$$

## Gamma

$$
k^{\star} = \frac{\beta^\alpha (\alpha-1)^{\alpha-1}e^{-(\alpha-1)}}{\lambda(\beta-\lambda)^{\alpha-1}\Gamma(\alpha)}
$$ ...math... $$
k^{\star} = \frac{\alpha^{\alpha}e^{-(\alpha-1)}}{\Gamma(\alpha)}
$$

## Gamma

```{r}
lambda <-1.5
alpha <- 2
kstar <- alpha^alpha*exp(-(alpha-1))/gamma(alpha)
```

### Gamme

```{r}
plot(seq(0,20,0.01),kstar*dexp(seq(0,20,0.01),rate = 0.1),type="l",col="red",ylim=c(0,0.25))
points(seq(0,20,0.01),dgamma(seq(0,20,0.01),rate = 0.1, shape = lambda),type="l",col="black")
```

## Gamma

```{r}
gamma.sim <- function(alpha, beta) {
# sim a gamma(lambda, m) rv using rejection with an exp envelope
# assumes m > 1 and lambda > 0
f <- function(x) beta^alpha*x^(alpha-1)*exp(-beta*x)/gamma(alpha)
h <- function(x) beta/alpha*exp(-beta/alpha*x)
k <- alpha^alpha*exp(1-alpha)/gamma(alpha)
while (TRUE) {
X <- -log(runif(1))*alpha/beta
Y <- runif(1, 0, k*h(X))
if (Y < f(X)) return(X)
}
}
```

## Gamma

```{r}
#| echo: false
set.seed(1999)
n <- 10000
g <- rep(0, n)
for (i in 1:n) g[i] <- gamma.sim(1, 2)
hist(g, breaks=20, freq=F, xlab="x", ylab="pdf f(x)",
main="theoretical and simulated gamma(1, 2) density")
x <- seq(0, max(g), .1)
lines(x, dgamma(x, 1, 2))
```

## Simulating Normals

-   Recall that if $Z\sim N(0,1)$ then $\mu + \sigma Z \sim N(\mu,\sigma^2)$.
-   So if we can sample from a $N(0,1)$, we can sample from any normal.
-   One really simple approach is to use the CLT.
-   Sample $n$ draws from $U(0,1)$ and the sum is approximately normal for large $n$.

## Simulating Normals

```{r}
set.seed(63793)
norm <- rep(NA, 100)
#What are 18 and 3 here?  
rnorm2 <- function(){
  out <- (sum(runif(36)) - 18) / sqrt(3)
  return(out)
}

norm <- replicate(10000,rnorm2())
```

## Simulating Normals

```{r}
qqnorm(norm)
```

## Simulating Normals

```{r}
hist(norm,freq=FALSE)
points(seq(-3,3,0.01),dnorm(seq(-3,3,0.01)),type="l",col="red")
```

## Simulating Normals

-   But we can do way better than this.\
-   We are going to use the rejection method with an exponential envelope.\
-   What's one obvious problem with using an exponential envelope to generate normal r.v.'s? (Think about support.)

## Simulating Normals

-   If $Z\sim N(0,1)$ then $X = |Z|$ has a so called \`\`half-normal" density.\
-   And if you define a r.v. $S$ to be $\pm 1$ with probability half each, then $Z = SX \sim N(0,1)$

## Simulating Normals

Half Normal Density $$
f_X(x) = \sqrt{\frac{2}{\pi}} e^{-\frac{1}{2}x^2} I(x\ge 0)
$$

## Simulating Normals

-   Consider using an $Exp(1)$ as the envelope function.\
-   Envelope density is $h(x) = e^{(-x)}$ for $x > 0$.

$$
k^{\star} = sup_x \frac{f_{X}(x)}{h(x)} = sup_x \sqrt{\frac{2}{\pi}} e^{x - \frac{x^2}{2}} = \sqrt{\frac{2e}{\pi}}
$$

## Simulating Normals

Standard Normal Simulation using Rejection

1 . Generate $X \sim Exp(1)$ and $Y \sim U(0, e^{-X}\sqrt{\frac{2e}{\pi}})$

2.  If $Y < \phi(X)$ then generate $S$ and return $SX$, otherwise go back to step 1. (Note: $\phi$ is the pdf of a standard normal)

## Simulating Normals

-   Slightly better way to generate Standard Normal Simulation using Rejection
-   Notice that $X$ is generated with the inversion method by $-log(U)$. This means that $U = e^{(-X)}$ and thus $Y \sim U(0, U \sqrt{\frac{2e}{\pi}})$.
-   Second, if $Y < \phi(X)$, then $Y < \frac{\phi(X)}{2}$ with probability 0.5.

## An even better way!

Slightly better way to generate Standard Normal Simulation using Rejection

1 . Generate $U \sim U(0,1)$ and $Y \sim U(0, U\sqrt{\frac{2e}{\pi}})$

2.  Put $X = -log(U)$
3.  Then

3a. If $Y < \frac{\phi(X)}{2}$ then return $Z = -X$

3b. If $\frac{\phi(X)}{2} < Y < \phi(X)$ then return $Z = X$

4.  Else return to step 1

## Code

```{r}
normsim <- function() {
  z <- NULL 
  while (is.null(z)){
  u <- runif(1)
  y <- runif(1, 0, u * sqrt(2 * exp(1) / pi))
  x <- -log(u)
  if (y < dnorm(x) / 2) {
    z <- -x
  } else if(y > dnorm(x) / 2 & y < dnorm(x)) {
    z <- x     
  }
  }
return(z)
  }

vec <- replicate(10000,normsim())


```

## 

```{r}
#| eval: false
sim <- replicate(10000, normsim())
hist(sim)

#ggplot histogram and wet noodle (i.e. density estimator)
ggsim <- data.frame(sim = sim)
ggplot(data = ggsim, aes(x = sim)) + geom_histogram(aes(y = ..density..), colour = "orange1") + geom_density(colour = "red4")

ggplot(data = ggsim, aes(x = sim)) + geom_freqpoly() + geom_density(colour = "red4")



qqnorm(sim)
qqline(sim, col = "red")
```
